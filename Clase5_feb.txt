Lectura de texto como cadena->
Limpiar de HTML->
Tokenizar:word_tokenize(),PlaintextCorpusReader,Split()->
Selección de los tokens relevantes para mi objetivo->
Vectorización de texto(conjunto de tokens o caracteristicas)
-ángulo entre vectores con coseno

Clase 7/02

Normalización de texto
	- Todo a minusculas
	- Eliminar stopwords
	- Eliminar los html tags
	- Eliminar todo caracter que no es letra

	Sustitución de formas morfologicas de una palabra por su lema
	(eats, ate ) => eat(lema)
	- Stemming
	- Lematización
	Libro:Text Analytics With Python
	    ->Text Normalization
	      (Pag 131 - 148)

4 palabras izq der de contexto
tenemos nuesto vocabulario
texto = lista de palabras normalizadas(pueden ser repetidas, sin ordenar)
gato_contexto = [ ... ] (4 izquierda 4 derecha)
^se puede hacer como diccionario
y queremos pasarlo a un vector

freeling tags
lookup tagger; nosotros mismos tenemos que encontrar las etiquetas más frecuentes y podemos variar la cantidad de palabras etiquetadas
unigramTagger: toma en cuenta todas las palabras
regexopTagger: encuentra patrones y etiqueta dependiendo de los patrones, si no encuentra usa un tagger por defecto

combinación de los 3
combined_tagger = nltk.unigramTagger()


Utilizando el programa vamos a hacer nuestro spanish tagger

Texto->
Normalización->
Selección de caracteristicas-> 
	Linguisticas
		Unigramas origiales
		Lemas con POS
		Stems
	Numericas
		1.frecuencia original
		2.frecuencia normalizada(probabilidad)
		3.term frequency(tf)
			tf(frecuencia original) = valor
			log(1 + x)
			y = (k + 1) x  / (x + k) 
		4.iverse document frequency(idf)
LancasterStemmer

RegexpStemmer
1. Stemming de Snowball + frec.orig
2. Stemming de Snowball + probabilidad

Problemas de Frecuencia
1-Longitud distinta de documentos
2-Palabras demasiado frecuentes en un doc
3-Palabras comunes
4-

Feature Extraction
gensim
word embeddings (se puede usar pero no se puede interpretar)
    se divide en varios modelos
    -word2vec(gensim.models.Word2Vec(params...)

Tarea1 para el martes:
    encontrar IDF-weighted dl = x1 * IDF*?(w1) ordenado
    con lemmatizer 

Randomness of X is measured by Entropy H(X)
Conditional Entropy
Conditional Entropy to capture syntagmatic relation

Tarea2 Conditional Entropy menos entropia mas relación sintamatica
se calculan por medio de las frecuencias de W1, W2 y W1 + W2
probabilidad conjunta es p(x) u p(y)
"parser" para analisis sintactico
Para la estimación de probabilidades tomaremos una oración como segmento, normalizamos cada oración(lemmatizar, eliminar stopwords, caracteres esp, minusculas) 
Filtrar a partir de un valor de H( ) umbral = threshold
Limitante de la entropia condicional

